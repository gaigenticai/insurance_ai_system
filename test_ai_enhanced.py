#!/usr/bin/env python3
"""
Enhanced AI integration test with advanced features and analytics.
Tests all new AI capabilities including analytics, enhanced prompts, and local LLM support.
"""

import asyncio
import json
import os
import sys
import time
from typing import Dict, Any

# Add the project root to the Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Set test environment variables
os.environ.setdefault("OPENAI_API_KEY", "test-key-openai")
os.environ.setdefault("ANTHROPIC_API_KEY", "test-key-anthropic")
os.environ.setdefault("AI_PROVIDER", "openai")
os.environ.setdefault("AI_MODEL", "gpt-3.5-turbo")
os.environ.setdefault("AI_ENABLE_FALLBACK", "true")
os.environ.setdefault("AI_ENABLE_METRICS", "true")

def test_enhanced_imports():
    """Test that all enhanced AI components can be imported."""
    print("üß™ Testing Enhanced AI Component Imports...")
    
    try:
        from ai_services.ai_service_manager import AIServiceManager
        print("‚úÖ Enhanced AI Service Manager")
        
        from ai_services.ai_analytics import AIMonitor, AIPerformanceTracker, get_ai_monitor
        print("‚úÖ AI Analytics and Monitoring")
        
        from ai_services.prompt_templates import AdvancedPromptTechniques, InsurancePromptEnhancer
        print("‚úÖ Advanced Prompt Engineering")
        
        from ai_services.llm_providers import LocalLLMProvider
        print("‚úÖ Enhanced Local LLM Provider")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Enhanced import failed: {e}")
        return False

def test_ai_analytics():
    """Test AI analytics and monitoring functionality."""
    print("\nüß™ Testing AI Analytics...")
    
    try:
        from ai_services.ai_analytics import AIMonitor, AIPerformanceTracker
        
        monitor = AIMonitor()
        
        # Test performance tracking
        with AIPerformanceTracker("openai", "gpt-3.5-turbo", "test_operation", monitor) as tracker:
            time.sleep(0.1)  # Simulate operation
            tracker.set_token_usage({"total_tokens": 150, "prompt_tokens": 100, "completion_tokens": 50})
            tracker.set_confidence_score(0.85)
        
        # Test analytics summary
        analytics = monitor.get_analytics_summary()
        assert analytics.total_requests >= 1
        print("‚úÖ Analytics summary generated")
        
        # Test provider comparison
        comparison = monitor.get_provider_comparison()
        print("‚úÖ Provider comparison generated")
        
        # Test error analysis
        error_analysis = monitor.get_error_analysis()
        print("‚úÖ Error analysis generated")
        
        # Test performance trends
        trends = monitor.get_performance_trends()
        print("‚úÖ Performance trends generated")
        
        # Test metrics export
        exported_metrics = monitor.export_metrics()
        assert isinstance(exported_metrics, str)
        print("‚úÖ Metrics export successful")
        
        return True
        
    except Exception as e:
        print(f"‚ùå AI analytics test failed: {e}")
        return False

def test_advanced_prompt_techniques():
    """Test advanced prompt engineering techniques."""
    print("\nüß™ Testing Advanced Prompt Techniques...")
    
    try:
        from ai_services.prompt_templates import AdvancedPromptTechniques, InsurancePromptEnhancer
        
        techniques = AdvancedPromptTechniques()
        enhancer = InsurancePromptEnhancer()
        
        base_prompt = "Analyze this insurance application for risk assessment."
        
        # Test chain-of-thought prompting
        cot_prompt = techniques.chain_of_thought_prompt(
            base_prompt,
            ["Analyze demographics", "Evaluate financial stability", "Assess risk factors"]
        )
        assert "step by step" in cot_prompt.lower()
        print("‚úÖ Chain-of-thought prompting")
        
        # Test few-shot prompting
        examples = [
            {"input": "35-year-old engineer", "output": "Low risk"},
            {"input": "55-year-old construction worker", "output": "Moderate risk"}
        ]
        few_shot_prompt = techniques.few_shot_prompt(base_prompt, examples)
        assert "examples" in few_shot_prompt.lower()
        print("‚úÖ Few-shot prompting")
        
        # Test role-based prompting
        role_prompt = techniques.role_based_prompt(
            base_prompt,
            "Senior Underwriter",
            ["risk assessment", "actuarial science"]
        )
        assert "expert" in role_prompt.lower()
        print("‚úÖ Role-based prompting")
        
        # Test constraint-based prompting
        constraints = {"format": "JSON", "confidence": "include scores"}
        constraint_prompt = techniques.constraint_based_prompt(base_prompt, constraints)
        assert "constraints" in constraint_prompt.lower()
        print("‚úÖ Constraint-based prompting")
        
        # Test multi-perspective prompting
        perspectives = ["underwriter", "actuary", "compliance"]
        multi_prompt = techniques.multi_perspective_prompt(base_prompt, perspectives)
        assert "perspectives" in multi_prompt.lower()
        print("‚úÖ Multi-perspective prompting")
        
        # Test enhanced insurance prompts
        test_data = {
            "applicant_name": "John Doe",
            "age": 35,
            "occupation": "Software Engineer"
        }
        
        enhanced_underwriting = enhancer.get_enhanced_underwriting_prompt(test_data)
        assert len(enhanced_underwriting) > len(base_prompt)
        print("‚úÖ Enhanced underwriting prompts")
        
        enhanced_claims = enhancer.get_enhanced_claims_prompt({"claim_id": "CLM-001"})
        assert "perspectives" in enhanced_claims.lower()
        print("‚úÖ Enhanced claims prompts")
        
        enhanced_actuarial = enhancer.get_enhanced_actuarial_prompt({"historical_data": {}})
        assert "step by step" in enhanced_actuarial.lower()
        print("‚úÖ Enhanced actuarial prompts")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Advanced prompt techniques test failed: {e}")
        return False

def test_local_llm_providers():
    """Test enhanced local LLM provider support."""
    print("\nüß™ Testing Enhanced Local LLM Providers...")
    
    try:
        from ai_services.llm_providers import LLMProviderFactory
        
        # Test Ollama provider
        ollama_config = {
            "model": "llama2:7b",
            "base_url": "http://localhost:11434",
            "provider_type": "ollama"
        }
        ollama_provider = LLMProviderFactory.create_provider("local", ollama_config)
        assert ollama_provider.provider_type == "ollama"
        print("‚úÖ Ollama provider created")
        
        # Test LM Studio provider
        lmstudio_config = {
            "model": "llama-2-7b-chat",
            "base_url": "http://localhost:1234",
            "provider_type": "lmstudio"
        }
        lmstudio_provider = LLMProviderFactory.create_provider("local", lmstudio_config)
        assert lmstudio_provider.provider_type == "lmstudio"
        print("‚úÖ LM Studio provider created")
        
        # Test Text Generation WebUI provider
        textgen_config = {
            "model": "llama-2-7b",
            "base_url": "http://localhost:5000",
            "provider_type": "textgen"
        }
        textgen_provider = LLMProviderFactory.create_provider("local", textgen_config)
        assert textgen_provider.provider_type == "textgen"
        print("‚úÖ Text Generation WebUI provider created")
        
        # Test llama.cpp provider
        llamacpp_config = {
            "model": "llama-2-7b-chat",
            "base_url": "http://localhost:8080",
            "provider_type": "llamacpp"
        }
        llamacpp_provider = LLMProviderFactory.create_provider("local", llamacpp_config)
        assert llamacpp_provider.provider_type == "llamacpp"
        print("‚úÖ llama.cpp provider created")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Local LLM providers test failed: {e}")
        return False

async def test_enhanced_ai_service_manager():
    """Test enhanced AI service manager functionality."""
    print("\nüß™ Testing Enhanced AI Service Manager...")
    
    try:
        from ai_services.ai_service_manager import AIServiceManager
        
        # Create mock AI service manager
        class MockAIServiceManager(AIServiceManager):
            def __init__(self):
                super().__init__()
                self._initialized = True
                
                # Mock provider
                class MockProvider:
                    def __init__(self):
                        self.model = "mock-model"
                    
                    async def generate_response(self, prompt, **kwargs):
                        from ai_services.llm_providers import AIResponse
                        return AIResponse(
                            content=json.dumps({
                                "analysis": "mock analysis",
                                "confidence": 0.85
                            }),
                            model="mock-model",
                            usage={"total_tokens": 100},
                            metadata={"provider": "mock"}
                        )
                
                self.providers = {"openai": MockProvider()}
                self.default_provider = self.providers["openai"]
                # Override settings to use mock provider
                self.settings.ai.provider = "openai"
        
        manager = MockAIServiceManager()
        
        # Test enhanced underwriting analysis
        underwriting_data = {
            "applicant_name": "John Doe",
            "age": 35,
            "occupation": "Software Engineer"
        }
        
        response = await manager.analyze_underwriting(
            underwriting_data,
            use_enhanced_prompts=True
        )
        assert response.content
        print("‚úÖ Enhanced underwriting analysis")
        
        # Test enhanced claims analysis
        claims_data = {
            "claim_id": "CLM-001",
            "claimed_amount": 2500
        }
        
        response = await manager.analyze_claims(
            claims_data,
            use_enhanced_prompts=True
        )
        assert response.content
        print("‚úÖ Enhanced claims analysis")
        
        # Test enhanced actuarial analysis
        actuarial_data = {
            "historical_data": {"claim_frequency": 0.15}
        }
        
        response = await manager.analyze_actuarial(
            actuarial_data,
            use_enhanced_prompts=True
        )
        assert response.content
        print("‚úÖ Enhanced actuarial analysis")
        
        # Test analytics functionality
        analytics = manager.get_ai_analytics()
        assert "analytics_summary" in analytics
        print("‚úÖ AI analytics integration")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Enhanced AI service manager test failed: {e}")
        return False

def test_new_api_endpoints():
    """Test new API endpoint models."""
    print("\nüß™ Testing New API Endpoints...")
    
    try:
        # Test that we can import and create new endpoint models
        from pydantic import BaseModel, Field
        from typing import Dict, Any, Optional
        from datetime import datetime
        
        class AIAnalyticsResponse(BaseModel):
            success: bool
            data: Optional[Dict[str, Any]] = None
            analytics_summary: Optional[Dict[str, Any]] = None
            provider_comparison: Optional[Dict[str, Any]] = None
            error_analysis: Optional[Dict[str, Any]] = None
            
        class BenchmarkRequest(BaseModel):
            test_prompt: Optional[str] = None
            providers: Optional[list] = None
            
        class BenchmarkResponse(BaseModel):
            success: bool
            benchmark_results: Dict[str, Any]
            test_prompt: str
            timestamp: float
        
        # Test creating instances
        analytics_response = AIAnalyticsResponse(
            success=True,
            analytics_summary={"total_requests": 100},
            provider_comparison={"openai": {"avg_response_time": 1.5}}
        )
        
        benchmark_request = BenchmarkRequest(
            test_prompt="Test insurance analysis",
            providers=["openai", "anthropic"]
        )
        
        benchmark_response = BenchmarkResponse(
            success=True,
            benchmark_results={"openai": {"success": True}},
            test_prompt="Test prompt",
            timestamp=time.time()
        )
        
        print("‚úÖ AI analytics endpoint models")
        print("‚úÖ Benchmark endpoint models")
        print("‚úÖ Provider comparison models")
        
        return True
        
    except Exception as e:
        print(f"‚ùå New API endpoints test failed: {e}")
        return False

async def test_performance_monitoring():
    """Test performance monitoring and tracking."""
    print("\nüß™ Testing Performance Monitoring...")
    
    try:
        from ai_services.ai_analytics import AIPerformanceTracker, get_ai_monitor
        
        monitor = get_ai_monitor()
        
        # Test multiple operations with tracking
        operations = [
            ("openai", "gpt-3.5-turbo", "underwriting"),
            ("anthropic", "claude-3-sonnet", "claims"),
            ("local", "llama2:7b", "actuarial")
        ]
        
        for provider, model, operation in operations:
            with AIPerformanceTracker(provider, model, operation) as tracker:
                await asyncio.sleep(0.05)  # Simulate operation
                tracker.set_token_usage({"total_tokens": 100})
                tracker.set_confidence_score(0.8)
        
        # Test analytics after operations
        analytics = monitor.get_analytics_summary()
        assert analytics.total_requests >= len(operations)
        print("‚úÖ Performance tracking")
        
        # Test provider comparison
        comparison = monitor.get_provider_comparison()
        print("‚úÖ Provider performance comparison")
        
        # Test model performance
        model_perf = monitor.get_model_performance()
        print("‚úÖ Model performance analysis")
        
        # Test trends
        trends = monitor.get_performance_trends()
        assert "hourly_trends" in trends
        print("‚úÖ Performance trends")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Performance monitoring test failed: {e}")
        return False

async def main():
    """Run all enhanced AI integration tests."""
    print("üöÄ Starting Enhanced AI Integration Tests")
    print("=" * 60)
    
    test_results = []
    
    # Run synchronous tests
    test_results.append(test_enhanced_imports())
    test_results.append(test_ai_analytics())
    test_results.append(test_advanced_prompt_techniques())
    test_results.append(test_local_llm_providers())
    test_results.append(test_new_api_endpoints())
    
    # Run asynchronous tests
    test_results.append(await test_enhanced_ai_service_manager())
    test_results.append(await test_performance_monitoring())
    
    # Summary
    passed_tests = sum(test_results)
    total_tests = len(test_results)
    
    print("\n" + "=" * 60)
    print(f"üìä Enhanced Test Results Summary:")
    print(f"Passed: {passed_tests}/{total_tests}")
    print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")
    
    if passed_tests == total_tests:
        print("üéâ All enhanced AI integration tests passed!")
        print("\n‚ú® Enhanced AI Features Successfully Tested:")
        print("  ‚Ä¢ Advanced prompt engineering techniques")
        print("  ‚Ä¢ Comprehensive AI analytics and monitoring")
        print("  ‚Ä¢ Enhanced local LLM provider support")
        print("  ‚Ä¢ Performance tracking and benchmarking")
        print("  ‚Ä¢ Multi-provider comparison and fallback")
        print("  ‚Ä¢ Real-time metrics and trend analysis")
        print("  ‚Ä¢ Enhanced API endpoints for analytics")
        
        print("\nüîß Enhanced Features Available:")
        print("  ‚Ä¢ Chain-of-thought prompting for better reasoning")
        print("  ‚Ä¢ Few-shot learning with examples")
        print("  ‚Ä¢ Role-based prompts for domain expertise")
        print("  ‚Ä¢ Multi-perspective analysis")
        print("  ‚Ä¢ Real-time performance monitoring")
        print("  ‚Ä¢ Provider benchmarking and comparison")
        print("  ‚Ä¢ Advanced local LLM support (Ollama, LM Studio, etc.)")
        print("  ‚Ä¢ Comprehensive analytics dashboard")
        
        print("\nüöÄ Next Steps:")
        print("  1. Configure your preferred AI provider (OpenAI/Anthropic/Local)")
        print("  2. Set up monitoring and analytics")
        print("  3. Test with real insurance data")
        print("  4. Explore advanced prompt techniques")
        print("  5. Set up local LLM for privacy-sensitive data")
        print("  6. Monitor performance and optimize costs")
        
        return True
    else:
        print(f"‚ö†Ô∏è {total_tests - passed_tests} enhanced tests failed")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)